{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "实现中的问题：\n",
    "１．　adaboost的思路很清晰，知道alpha和ｗ的计算公式后，整个算法很好描述\n",
    "难点反而在弱分类器那里，sklearn里一般用ＣＡＲＴ决策树，我们这里用的是树桩，即只有三个节点的二叉树，只切分一次\n",
    "按理说，应该可以直接拿已经实现的决策树作为这里的弱分类器，这里没有尝试。\n",
    "\n",
    "２．　坑都在弱分类器的实现里，选择最优特征的最优二值点，记录error,yp等。\n",
    "这里有一个learning rate，用来作为切分特征值的步长\n",
    "\n",
    "３．　原代码，在弱分类器中设置了停止寻找特征的条件，即error＝＝０\n",
    "这里，没有做这样的设置，而是在每次的弱分类器确定后，统一计算了当前分类器的error，如果==0,则提前结束，不必等执行完所有的n_estimator.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from collections import Counter\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load data与感知机所用相同\n",
    "data=load_iris()\n",
    "# print(data)\n",
    "df=pd.DataFrame(data.data,columns=data.feature_names)\n",
    "df['label']=data.target\n",
    "df.columns=['sl','sw','pl','pw','label']\n",
    "\n",
    "# X,y\n",
    "X=np.array(df.iloc[:100,[0,1]])\n",
    "y=np.array(df.iloc[:100,-1])\n",
    "\n",
    "y=np.array([-1 if i==0 else 1 for i in y  ])\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AdaBoostClassifier():\n",
    "    def __init__(self,n_estimator=50,lr=0.5):\n",
    "        self.n_estimator=n_estimator\n",
    "        self.lr=lr\n",
    "    \n",
    "    def _init_params(self,X_train,y_train):\n",
    "        self.X=X_train\n",
    "        self.y=y_train\n",
    "        l=len(self.X)\n",
    "        self.D=[1/l]*l\n",
    "        self.clf_alpha=[]\n",
    "    \n",
    "    \n",
    "    def _alpha(self,error):\n",
    "        return 0.5*math.log((1-error)/error)\n",
    "    \n",
    "    def _w_update(self,D:list,yp:list,alpha):\n",
    "        wp=[]\n",
    "        Z=sum([D[i]*math.exp(-alpha*self.y[i]*yp[i]) for i in range(len(D))])\n",
    "        \n",
    "        for i in range(len(D)):\n",
    "            wp.append(D[i]*math.exp(-alpha*self.y[i]*yp[i])/Z)\n",
    "        return wp\n",
    "    \n",
    "    def ferror(self):\n",
    "        fyp=[0.0]*len(self.y)\n",
    "        for alpha,bc in self.clf_alpha:\n",
    "            fyp+=alpha*np.array(bc.yp)\n",
    "        fyp=[1 if i>0 else -1 for i in fyp]\n",
    "        return sum([0 if fyp[i]==self.y[i] else 1 for i in range(len(fyp))])/len(self.y)\n",
    "            \n",
    "    def fit(self,X_train,y_train):\n",
    "        self._init_params(X_train,y_train)\n",
    "        for i in range(self.n_estimator):\n",
    "            bc=BasicClassifier(w=self.D,lr=self.lr)\n",
    "            bc.fit(X_train,y_train)\n",
    "            alpha=self._alpha(bc.error)\n",
    "            self.D=self._w_update(self.D,bc.yp,alpha)\n",
    "            self.clf_alpha.append((alpha,bc))\n",
    "            if self.ferror()==0:\n",
    "                break\n",
    "    \n",
    "    def score(self,X_test,y_test):\n",
    "        yp=[self.predict(xp) for xp in X_test]\n",
    "        return sum([1 if yp[i]==y_test[i] else 0 for i in range(len(yp))])/len(y_test)\n",
    "        \n",
    "    def predict(self,xp):\n",
    "        yp=0\n",
    "        for alpha,bc in self.clf_alpha:\n",
    "            yp+=alpha*bc.predict(xp)\n",
    "        if yp>0:\n",
    "            return 1\n",
    "        else:\n",
    "            return -1\n",
    "            \n",
    "class BasicClassifier():\n",
    "    def __init__(self,w,lr=0.5):\n",
    "        self.lr=lr\n",
    "        self.w=w\n",
    "        self.fi=None\n",
    "        self.vi=None\n",
    "        self.direction=None\n",
    "        self.error=math.inf\n",
    "        self.yp=[]\n",
    "    def fit(self,X_train,y_train):\n",
    "        X_features=list(zip(*X_train))\n",
    "        for i in range(len(X_train[0])):\n",
    "            min_f=min(X_features[i])\n",
    "            max_f=max(X_features[i])\n",
    "            n_step=(max_f-min_f+self.lr)//self.lr\n",
    "            \n",
    "            for j in range(int(n_step)):\n",
    "                error=math.inf\n",
    "                direct=None\n",
    "                yp_array=[]\n",
    "                v=min_f+j*self.lr\n",
    "                pos_array=[1 if f>=v else -1 for f in X_features[i]]\n",
    "                pos_error=sum([self.w[e] if pos_array[e]!=y_train[e] else 0 for e in range(len(pos_array))])\n",
    "                neg_array=[-1 if f>=v else 1 for f in X_features[i]]\n",
    "                neg_error=sum([self.w[e] if neg_array[e]!=y_train[e] else 0 for e in range(len(neg_array))])\n",
    "                \n",
    "                if pos_error>neg_error:\n",
    "                    error=neg_error\n",
    "                    yp_array=neg_array\n",
    "                    direct='negtive'\n",
    "                else:\n",
    "                    error=pos_error\n",
    "                    yp_array=pos_array\n",
    "                    direct='postive'\n",
    "                    \n",
    "                if error<self.error:\n",
    "                    self.fi=i\n",
    "                    self.yp=yp_array\n",
    "                    self.error=error\n",
    "                    self.vi=v\n",
    "                    self.direction=direct\n",
    "    def fx(self,x,direct,fi,vi):\n",
    "        if direct=='postive':\n",
    "            \n",
    "            if x[fi]>=vi:\n",
    "                return 1\n",
    "            else:\n",
    "                return -1\n",
    "        else:\n",
    "            if x[fi]>=vi:\n",
    "                return -1\n",
    "            else:\n",
    "                return 1\n",
    "        \n",
    "    def predict(self,xp):\n",
    "        return self.fx(xp,self.direction,self.fi,self.vi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada=AdaBoostClassifier()\n",
    "ada.fit(X_train,y_train)\n",
    "ada.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "clf=AdaBoostClassifier(n_estimators=10,learning_rate=0.2,algorithm='SAMME.R')\n",
    "clf.fit(X_train,y_train)\n",
    "clf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
